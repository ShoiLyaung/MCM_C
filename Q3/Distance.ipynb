{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\n词形还原\\nfrom nltk.stem import WordNetLemmatizer\\nlemmatizer = WordNetLemmatizer()\\naa = lemmatizer.lemmatize('recognitions')\\nprint(aa)\\n\""
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "VALUE = 0.9#\n",
    "METHOD = 'leven'\n",
    "\n",
    "def similarity(word1, word2, method):\n",
    "    if method == 'edit':\n",
    "        return editSim(word1, word2)\n",
    "    elif method == 'hamming':\n",
    "        return hammingSim(word1, word2)\n",
    "    elif method == 'leven':\n",
    "        return levenSim(word1, word2)\n",
    "    elif method == 'jaro':\n",
    "        return jaroSim(word1, word2)\n",
    "    elif method == 'jaro_winkler':\n",
    "        return jaroWinklerSim(word1, word2)\n",
    "    elif method == 'lcs':\n",
    "        return lcsSim(word1, word2)\n",
    "    elif method == 'dice':\n",
    "        return diceSim(word1, word2)\n",
    "    elif method == 'wordnet':\n",
    "        return wordnetSim(word1, word2)\n",
    "\n",
    "def editSim(word1, word2):\n",
    "    \"\"\"\n",
    "    按照编辑距离，计算两个词的编辑相似度。单纯使用编辑距离/较长词\n",
    "    :param word1: 词\n",
    "    :param word2: 词\n",
    "    :return: 编辑相似度\n",
    "    \"\"\"\n",
    "    n = max(len(word1), len(word2))\n",
    "    return 1-Levenshtein.distance(word1, word2)/n\n",
    "\n",
    "def hammingSim(word1, word2):\n",
    "    \"\"\"\n",
    "    按照汉明距离，计算两个词的汉明相似度。单纯使用汉明距离 / 较长词。由于汉明距离计算要求两个word的长度必须一致，因此对长的词采用单纯的截断的方式\n",
    "    从前面截断、从后面截断\n",
    "    :param word1: 词\n",
    "    :param word2: 词\n",
    "    :return: 汉明相似度\n",
    "    \"\"\"\n",
    "    #从前面截断\n",
    "    n = max(len(word1), len(word2))\n",
    "    if len(word1) > len(word2):\n",
    "        word3 = word1[:len(word2)]\n",
    "        return (len(word2) - Levenshtein.hamming(word3, word2)) / n\n",
    "    else:\n",
    "        word3 = word2[:len(word1)]\n",
    "        return (len(word1) - Levenshtein.hamming(word1, word3)) / n\n",
    "'''\n",
    "    #从后面截断\n",
    "    n = max(len(word1), len(word2))\n",
    "    if len(word1) > len(word2):\n",
    "        word3 = word1[len(word1)-len(word2):]\n",
    "        return (len(word2) - Levenshtein.hamming(word3, word2)) / n\n",
    "    else:\n",
    "        word3 = word2[len(word2)-len(word1):]\n",
    "        return (len(word1) - Levenshtein.hamming(word1, word3)) / n\n",
    "'''\n",
    "\n",
    "def levenSim(word1, word2):\n",
    "    \"\"\"\n",
    "    计算莱文斯坦比。计算公式r=(sum–ldist)/sum,其中sum是指word1和word2字串的长度总和，ldist是类编辑距离。\n",
    "    注意这里是类编辑距离，不是通常所说的编辑距离，在类编辑距离中删除、插入依然+1，但是替换+2\n",
    "    :param word1: 词\n",
    "    :param word2: 词\n",
    "    :return: 莱文斯坦比\n",
    "    \"\"\"\n",
    "    return Levenshtein.ratio(word1, word2)\n",
    "\n",
    "def jaroSim(word1, word2):\n",
    "    \"\"\"\n",
    "    计算jaro距离。\n",
    "    :param word1: 词\n",
    "    :param word2: 词\n",
    "    :return: jaro距离\n",
    "    \"\"\"\n",
    "    return Levenshtein.jaro(word1, word2)\n",
    "\n",
    "def jaroWinklerSim(word1, word2):\n",
    "    \"\"\"\n",
    "    计算Jaro–Winkler距离，而Jaro-Winkler则给予了起始部分就相同的字符串更高的分数\n",
    "    :param word1: 词\n",
    "    :param word2: 词\n",
    "    :return: Jaro–Winkler距离\n",
    "    \"\"\"\n",
    "    return Levenshtein.jaro_winkler(word1, word2)\n",
    "\n",
    "def lcsSim(word1, word2):\n",
    "    \"\"\"\n",
    "    value越小，速度越快（value=0.5时，时间=5~10min，慢）\n",
    "    LCS，计算两个词的最长公共子序列长度。单纯使用lcs/较长词。\n",
    "    :param word1: 词\n",
    "    :param word2: 词\n",
    "    :return: LCS/n\n",
    "    \"\"\"\n",
    "    n = max(len(word1), len(word2))\n",
    "    dp = [[0 for i in range(len(word2)+1)] for j in range(len(word1)+1)]\n",
    "    for i in range(len(word1)-1, -1, -1):#倒序，简化边界条件判断\n",
    "        for j in range(len(word2)-1, -1, -1):\n",
    "            if word1[i] == word2[j]:\n",
    "                dp[i][j] = dp[i+1][j+1]+1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i+1][j], dp[i][j+1])\n",
    "    return dp[0][0]/n\n",
    "\n",
    "def diceSim(word1, word2):\n",
    "    \"\"\"\n",
    "    计算Dice距离，其用于度量两个集合的相似性，因为可以把字符串理解为一种集合，因此Dice距离也会用于度量字符串的相似性。\n",
    "    此外，Dice系数的一个非常著名的使用即实验性能评测的F1值\n",
    "    :param word1: 词\n",
    "    :param word2: 词\n",
    "    :return: Dice距离\n",
    "    \"\"\"\n",
    "    a_bigrams = set(word1)\n",
    "    b_bigrams = set(word2)\n",
    "    overlap = len(a_bigrams & b_bigrams)\n",
    "    return overlap*2.0/(len(a_bigrams)+len(b_bigrams))\n",
    "\n",
    "def wordnetSim(word1, word2):##有问题？\n",
    "    \"\"\"\n",
    "    语义相似度，基于wordnet\n",
    "    计算词语相似度时，总以包含词语概念的语义相似度的最大值来表示词语的语义相似度，将其也应用于短文本（短语），即基于最大值的短文本语义相似度\n",
    "    ##对于短语中部分词相等的处理\n",
    "    :param word1: 词\n",
    "    :param word2: 词\n",
    "    :return: 基于最大值的wordnet的语义相似度\n",
    "    \"\"\"\n",
    "    phrase1 = word1\n",
    "    phrase2 = word2\n",
    "    word1 = phrase1.split(' ')\n",
    "    word2 = phrase2.split(' ')\n",
    "    path_sim = 0\n",
    "    for w1 in word1:\n",
    "        for w2 in word2:\n",
    "            synsets1 = wn.synsets(w1)\n",
    "            synsets2 = wn.synsets(w2)\n",
    "            #path_sim = 0\n",
    "            for tmpword1 in synsets1:\n",
    "                for tmpword2 in synsets2:\n",
    "                    if tmpword1.pos() == tmpword2.pos():\n",
    "                        try:   ###对于短语中部分词相等的处理\n",
    "                            sim = tmpword1.path_similarity(tmpword2)\n",
    "                            if w1 != w2:  # 对于短语来说，不能将其中一个词相等，则短语完全相等\n",
    "                                path_sim = max(path_sim, sim)  # 取最大值\n",
    "                        except Exception as e:\n",
    "                            continue\n",
    "                            #print(tmpword1, tmpword2)\n",
    "                            #print(\"path: \" + str(e))\n",
    "    return path_sim\n",
    "\"\"\"\n",
    "词形还原\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "aa = lemmatizer.lemmatize('recognitions')\n",
    "print(aa)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('Wordle.csv')\n",
    "# print(training_set)\n",
    "X = training_set.iloc[:,2:3].values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "methods=['edit',\n",
    "'hamming',\n",
    "'leven',\n",
    "'jaro',\n",
    "'jaro_winkler',\n",
    "'lcs',\n",
    "'dice']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "with open('dists.csv','w+') as f:\n",
    "    for m in methods:\n",
    "        print('method: ',m,'results: ','\\n',file=f)\n",
    "        for i in range(len(X)):\n",
    "            for j in range(len(X)):\n",
    "                dist = similarity(X[i][0] , X[j][0] , m)\n",
    "                print(X[i][0],',',X[j][0],',', dist,file=f)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
